{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-23T17:01:51.891481Z","iopub.status.busy":"2024-07-23T17:01:51.890815Z","iopub.status.idle":"2024-07-23T17:01:53.189979Z","shell.execute_reply":"2024-07-23T17:01:53.188768Z","shell.execute_reply.started":"2024-07-23T17:01:51.891445Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/datapdf/how to talk anyone.pdf\n","/kaggle/input/datapdf/Never-Split-the-Difference.pdf\n","/kaggle/input/datapdf/The_Art_of_Public_Speaking-Dale_Carnegie.pdf\n","/kaggle/input/pdffff/WHAT TO SAY WHEN YOU TALK TO YOURSELF.pdf\n","/kaggle/input/pdf-data/Knowledge-Based Social Entrepreneurship.pdf\n","/kaggle/input/pdf-data/Silicon Valley North A High-Tech Cluster.pdf\n","/kaggle/input/pdf-data/The Lean Startup How Todays Entrepreneurs.pdf\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Installing the PyMuPDF library to extract content from PDF files."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:02:19.547069Z","iopub.status.busy":"2024-07-23T17:02:19.546260Z","iopub.status.idle":"2024-07-23T17:02:36.319006Z","shell.execute_reply":"2024-07-23T17:02:36.317737Z","shell.execute_reply.started":"2024-07-23T17:02:19.547035Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting PyMuPDF\n","  Downloading PyMuPDF-1.24.8-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting PyMuPDFb==1.24.8 (from PyMuPDF)\n","  Downloading PyMuPDFb-1.24.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n","Downloading PyMuPDF-1.24.8-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading PyMuPDFb-1.24.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n","Successfully installed PyMuPDF-1.24.8 PyMuPDFb-1.24.8\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install PyMuPDF\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:03:15.036633Z","iopub.status.busy":"2024-07-23T17:03:15.036218Z","iopub.status.idle":"2024-07-23T17:03:18.610430Z","shell.execute_reply":"2024-07-23T17:03:18.609256Z","shell.execute_reply.started":"2024-07-23T17:03:15.036591Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/datapdf/Never-Split-the-Difference.pdf: 375 pages extracted.\n","/kaggle/input/datapdf/how to talk anyone.pdf: 364 pages extracted.\n","/kaggle/input/pdffff/WHAT TO SAY WHEN YOU TALK TO YOURSELF.pdf: 258 pages extracted.\n"]}],"source":["import fitz  # PyMuPDF\n","\n","# Function to extract content from a PDF\n","def extract_content_from_pdf(pdf_path):\n","    document = fitz.open(pdf_path)\n","    content = []\n","    for page_num in range(len(document)):\n","        page = document.load_page(page_num)\n","        content.append(page.get_text())\n","    return content\n","\n","# Paths to the provided PDFs\n","pdf_paths = [\n","    \"/kaggle/input/datapdf/Never-Split-the-Difference.pdf\",\n","    \"/kaggle/input/datapdf/how to talk anyone.pdf\",\n","    \"/kaggle/input/pdffff/WHAT TO SAY WHEN YOU TALK TO YOURSELF.pdf\"\n","]\n","\n","# Extract content from each PDF\n","pdf_contents = {}\n","for pdf_path in pdf_paths:\n","    pdf_contents[pdf_path] = extract_content_from_pdf(pdf_path)\n","\n","# Display the number of pages extracted from each PDF for verification\n","for pdf_path, content in pdf_contents.items():\n","    print(f\"{pdf_path}: {len(content)} pages extracted.\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Defining the TreeNode class to represent nodes in the hierarchical tree"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:03:31.850897Z","iopub.status.busy":"2024-07-23T17:03:31.850450Z","iopub.status.idle":"2024-07-23T17:03:31.920369Z","shell.execute_reply":"2024-07-23T17:03:31.919459Z","shell.execute_reply.started":"2024-07-23T17:03:31.850862Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","class TreeNode:\n","    def __init__(self, id, name, parent=None):\n","        self.id = id\n","        self.name = name\n","        self.parent = parent\n","        self.children = []\n","        self.content = []\n","\n","    def add_child(self, node):\n","        self.children.append(node)\n","        node.parent = self\n","\n","    def add_content(self, text):\n","        self.content.append(text)\n","\n","    def to_dict(self):\n","        return {\n","            \"id\": self.id,\n","            \"name\": self.name,\n","            \"parent\": self.parent.id if self.parent else None,\n","            \"children\": [child.to_dict() for child in self.children],\n","            \"content\": self.content\n","        }\n","\n","    def __repr__(self):\n","        return f\"TreeNode(id={self.id}, name={self.name})\"\n","\n","# Function to analyze structure and create hierarchical tree\n","def create_tree_from_content(content):\n","    root = TreeNode(id=\"root\", name=\"Textbook\")\n","    current_node = root\n","    node_id = 1\n","    \n","    for page in content:\n","        for line in page.split('\\n'):\n","            if line.startswith(\"Chapter\") or line.startswith(\"CHAPTER\"):\n","                chapter_node = TreeNode(id=f\"chapter_{node_id}\", name=line)\n","                root.add_child(chapter_node)\n","                current_node = chapter_node\n","                node_id += 1\n","            elif line.startswith(\"Section\") or line.startswith(\"SECTION\"):\n","                section_node = TreeNode(id=f\"section_{node_id}\", name=line, parent=current_node)\n","                current_node.add_child(section_node)\n","                current_node = section_node\n","                node_id += 1\n","            elif line.strip():\n","                current_node.add_content(line)\n","    \n","    return root\n","\n","# Create hierarchical trees for each PDF\n","pdf_trees = {}\n","for pdf_path, content in pdf_contents.items():\n","    pdf_trees[pdf_path] = create_tree_from_content(content)\n","\n","# Convert trees to dictionaries and then to JSON for storage\n","pdf_trees_json = {pdf_path: json.dumps(tree.to_dict()) for pdf_path, tree in pdf_trees.items()}\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Storing hierarchical tree structures in an SQLite database\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:03:51.547071Z","iopub.status.busy":"2024-07-23T17:03:51.546274Z","iopub.status.idle":"2024-07-23T17:03:51.589583Z","shell.execute_reply":"2024-07-23T17:03:51.588531Z","shell.execute_reply.started":"2024-07-23T17:03:51.547038Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Hierarchical tree structures have been stored in the database.\n"]}],"source":["import sqlite3\n","\n","# Create an SQLite database to store the hierarchical tree structures\n","conn = sqlite3.connect('/kaggle/working/textbook_hierarchical_index.db')\n","cursor = conn.cursor()\n","\n","# Create a table to store the tree data\n","cursor.execute('''\n","    CREATE TABLE IF NOT EXISTS trees (\n","        id INTEGER PRIMARY KEY,\n","        pdf_path TEXT UNIQUE,\n","        tree_structure TEXT\n","    )\n","''')\n","\n","# Insert the hierarchical tree structures into the database\n","for pdf_path, tree_json in pdf_trees_json.items():\n","    cursor.execute('''\n","        INSERT OR REPLACE INTO trees (pdf_path, tree_structure) VALUES (?, ?)\n","    ''', (pdf_path, tree_json))\n","\n","conn.commit()\n","conn.close()\n","\n","print(\"Hierarchical tree structures have been stored in the database.\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Install the Whoosh library for indexing and searching text"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:04:12.948724Z","iopub.status.busy":"2024-07-23T17:04:12.947976Z","iopub.status.idle":"2024-07-23T17:04:26.948480Z","shell.execute_reply":"2024-07-23T17:04:26.947293Z","shell.execute_reply.started":"2024-07-23T17:04:12.948679Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting whoosh\n","  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n","Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: whoosh\n","Successfully installed whoosh-2.7.4\n"]}],"source":["!pip install whoosh\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:04:36.559446Z","iopub.status.busy":"2024-07-23T17:04:36.558585Z","iopub.status.idle":"2024-07-23T17:04:39.419048Z","shell.execute_reply":"2024-07-23T17:04:39.418234Z","shell.execute_reply.started":"2024-07-23T17:04:36.559410Z"},"trusted":true},"outputs":[],"source":["from whoosh import index\n","from whoosh.fields import Schema, TEXT, ID\n","from whoosh.qparser import QueryParser\n","import os\n","\n","# Define the schema for the index\n","schema = Schema(node_id=ID(stored=True), content=TEXT)\n","\n","# Create the index directory if it doesn't exist\n","if not os.path.exists(\"indexdir\"):\n","    os.mkdir(\"indexdir\")\n","\n","# Create the index\n","ix = index.create_in(\"indexdir\", schema)\n","writer = ix.writer()\n","\n","# Assuming pdf_trees is a dictionary where keys are PDF paths and values are root nodes of the hierarchical tree\n","def index_tree(writer, node):\n","    writer.add_document(node_id=node.id, content=\" \".join(node.content))\n","    for child in node.children:\n","        index_tree(writer, child)\n","\n","# Index the hierarchical trees\n","for pdf_path, tree in pdf_trees.items():\n","    index_tree(writer, tree)\n","\n","writer.commit()\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Install the Faiss library with GPU support"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:04:44.658027Z","iopub.status.busy":"2024-07-23T17:04:44.657380Z","iopub.status.idle":"2024-07-23T17:05:01.311770Z","shell.execute_reply":"2024-07-23T17:05:01.310444Z","shell.execute_reply.started":"2024-07-23T17:04:44.657970Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install faiss-gpu\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:10:29.756558Z","iopub.status.busy":"2024-07-23T18:10:29.755818Z","iopub.status.idle":"2024-07-23T18:10:42.676541Z","shell.execute_reply":"2024-07-23T18:10:42.675329Z","shell.execute_reply.started":"2024-07-23T18:10:29.756528Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n"]}],"source":["!pip install nltk transformers\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:19:46.754740Z","iopub.status.busy":"2024-07-23T18:19:46.753891Z","iopub.status.idle":"2024-07-23T18:19:46.761560Z","shell.execute_reply":"2024-07-23T18:19:46.760618Z","shell.execute_reply.started":"2024-07-23T18:19:46.754686Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'/kaggle/input/datapdf/Never-Split-the-Difference.pdf': TreeNode(id=root, name=Textbook),\n"," '/kaggle/input/datapdf/how to talk anyone.pdf': TreeNode(id=root, name=Textbook),\n"," '/kaggle/input/pdffff/WHAT TO SAY WHEN YOU TALK TO YOURSELF.pdf': TreeNode(id=root, name=Textbook)}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["pdf_trees"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:22:19.297838Z","iopub.status.busy":"2024-07-23T18:22:19.297227Z","iopub.status.idle":"2024-07-23T18:22:32.673426Z","shell.execute_reply":"2024-07-23T18:22:32.672290Z","shell.execute_reply.started":"2024-07-23T18:22:19.297807Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rank-bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rank-bm25) (1.26.4)\n","Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Installing collected packages: rank-bm25\n","Successfully installed rank-bm25-0.2.2\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["!pip install rank-bm25\n","\n","import sqlite3\n","import json\n","from whoosh import index\n","from whoosh.fields import Schema, TEXT, ID\n","from whoosh.qparser import QueryParser\n","import os\n","import faiss\n","import numpy as np\n","import nltk\n","from nltk.corpus import wordnet\n","from nltk.stem import PorterStemmer\n","from transformers import (\n","    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n","    DPRContextEncoder, DPRContextEncoderTokenizer,\n","    AutoModelForSequenceClassification, AutoTokenizer,\n","    T5ForConditionalGeneration, T5Tokenizer\n",")\n","from rank_bm25 import BM25Okapi\n","\n","# Download required NLTK data\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Load hierarchical tree structures from an SQLite database"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:23:16.046628Z","iopub.status.busy":"2024-07-23T18:23:16.046235Z","iopub.status.idle":"2024-07-23T18:23:16.067871Z","shell.execute_reply":"2024-07-23T18:23:16.066956Z","shell.execute_reply.started":"2024-07-23T18:23:16.046592Z"},"trusted":true},"outputs":[],"source":["def load_trees_from_db():\n","    conn = sqlite3.connect('/kaggle/working/textbook_hierarchical_index.db')\n","    cursor = conn.cursor()\n","    cursor.execute('SELECT pdf_path, tree_structure FROM trees')\n","    trees = {row[0]: json.loads(row[1]) for row in cursor.fetchall()}\n","    conn.close()\n","    return trees\n","\n","pdf_trees = load_trees_from_db()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Index hierarchical tree structures from database using Whoosh"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:23:28.908811Z","iopub.status.busy":"2024-07-23T18:23:28.908413Z","iopub.status.idle":"2024-07-23T18:23:31.536418Z","shell.execute_reply":"2024-07-23T18:23:31.535564Z","shell.execute_reply.started":"2024-07-23T18:23:28.908782Z"},"trusted":true},"outputs":[],"source":["schema = Schema(node_id=ID(stored=True), content=TEXT)\n","if not os.path.exists(\"indexdir\"):\n","    os.mkdir(\"indexdir\")\n","ix = index.create_in(\"indexdir\", schema)\n","\n","def index_tree(writer, node):\n","    writer.add_document(node_id=node['id'], content=\" \".join(node['content']))\n","    for child in node['children']:\n","        index_tree(writer, child)\n","\n","writer = ix.writer()\n","for pdf_path, tree in pdf_trees.items():\n","    index_tree(writer, tree)\n","writer.commit()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Encode hierarchical tree nodes using DPR and create a FAISS index for context embeddings"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:23:40.532706Z","iopub.status.busy":"2024-07-23T18:23:40.532315Z","iopub.status.idle":"2024-07-23T18:24:17.047796Z","shell.execute_reply":"2024-07-23T18:24:17.046874Z","shell.execute_reply.started":"2024-07-23T18:23:40.532662Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n","- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n","- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n","The class this function is called from is 'DPRContextEncoderTokenizer'.\n"]}],"source":["question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n","question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n","context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n","context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n","\n","context_embeddings = []\n","node_ids = []\n","\n","def encode_context(node):\n","    inputs = context_tokenizer(\" \".join(node['content']), return_tensors='pt', truncation=True, max_length=512)\n","    embeddings = context_encoder(**inputs).pooler_output.detach().numpy()\n","    context_embeddings.append(embeddings)\n","    node_ids.append(node['id'])\n","    for child in node['children']:\n","        encode_context(child)\n","\n","for pdf_path, tree in pdf_trees.items():\n","    encode_context(tree)\n","\n","context_embeddings = np.concatenate(context_embeddings, axis=0)\n","faiss_index = faiss.IndexFlatL2(context_embeddings.shape[1])\n","faiss_index.add(context_embeddings)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Expand and stem user queries using synonyms and stemming"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:24:32.015375Z","iopub.status.busy":"2024-07-23T18:24:32.014530Z","iopub.status.idle":"2024-07-23T18:24:32.022440Z","shell.execute_reply":"2024-07-23T18:24:32.021496Z","shell.execute_reply.started":"2024-07-23T18:24:32.015339Z"},"trusted":true},"outputs":[],"source":["stemmer = PorterStemmer()\n","\n","def expand_query_with_synonyms(query):\n","    expanded_terms = set(query.split())\n","    for word in query.split():\n","        for syn in wordnet.synsets(word):\n","            for lemma in syn.lemmas():\n","                expanded_terms.add(lemma.name())\n","    return \" \".join(expanded_terms)\n","\n","def stem_query(query):\n","    return \" \".join([stemmer.stem(word) for word in query.split()])\n","\n","def expand_and_stem_query(query):\n","    expanded_query = expand_query_with_synonyms(query)\n","    stemmed_query = stem_query(expanded_query)\n","    return stemmed_query"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Setup BM25 for hierarchical tree content retrieval"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:39:59.636755Z","iopub.status.busy":"2024-07-23T18:39:59.636049Z","iopub.status.idle":"2024-07-23T18:39:59.787282Z","shell.execute_reply":"2024-07-23T18:39:59.786526Z","shell.execute_reply.started":"2024-07-23T18:39:59.636721Z"},"trusted":true},"outputs":[],"source":["def setup_bm25(trees):\n","    corpus = []\n","    for tree in trees.values():\n","        def extract_content(node):\n","            content = \" \".join(node['content'])\n","            for child in node['children']:\n","                content += \" \" + extract_content(child)\n","            return content\n","        corpus.append(extract_content(tree))\n","    tokenized_corpus = [doc.split() for doc in corpus]\n","    return BM25Okapi(tokenized_corpus)\n","\n","bm25 = setup_bm25(pdf_trees)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Hybrid retrieval using DPR, BM25, and Whoosh\n"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:40:04.072074Z","iopub.status.busy":"2024-07-23T18:40:04.071277Z","iopub.status.idle":"2024-07-23T18:40:04.079979Z","shell.execute_reply":"2024-07-23T18:40:04.078942Z","shell.execute_reply.started":"2024-07-23T18:40:04.072043Z"},"trusted":true},"outputs":[],"source":["def hybrid_retrieval(query, k=10):\n","    expanded_query = expand_and_stem_query(query)\n","    \n","    # DPR retrieval\n","    question_input = question_tokenizer(expanded_query, return_tensors='pt')\n","    question_embedding = question_encoder(**question_input).pooler_output.detach().numpy()\n","    dpr_scores, dpr_indices = faiss_index.search(question_embedding, k)\n","    \n","    # BM25 retrieval\n","    bm25_scores = bm25.get_scores(expanded_query.split())\n","    bm25_indices = np.argsort(bm25_scores)[::-1][:k]\n","    \n","    # Whoosh retrieval\n","    with ix.searcher() as searcher:\n","        whoosh_query = QueryParser(\"content\", ix.schema).parse(expanded_query)\n","        whoosh_results = searcher.search(whoosh_query, limit=k)\n","        whoosh_indices = [int(hit['node_id']) for hit in whoosh_results]\n","    \n","    # Combine results\n","    combined_indices = list(set(dpr_indices[0].tolist() + bm25_indices.tolist() + whoosh_indices))\n","    return [node_ids[i] for i in combined_indices[:k]]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Rerank retrieved results using a cross-encoder model"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:40:14.658991Z","iopub.status.busy":"2024-07-23T18:40:14.658410Z","iopub.status.idle":"2024-07-23T18:40:15.048432Z","shell.execute_reply":"2024-07-23T18:40:15.047595Z","shell.execute_reply.started":"2024-07-23T18:40:14.658951Z"},"trusted":true},"outputs":[],"source":["reranker_model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n","reranker_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n","\n","def rerank_results(query, retrieved_contents):\n","    pairs = [[query, content] for content in retrieved_contents]\n","    inputs = reranker_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n","    scores = reranker_model(**inputs).logits.squeeze(-1)\n","    reranked_indices = scores.argsort(descending=True)\n","    return [retrieved_contents[i] for i in reranked_indices]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Generate answers using a RAG pipeline with T5 for conditional generation"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:40:33.111384Z","iopub.status.busy":"2024-07-23T18:40:33.110572Z","iopub.status.idle":"2024-07-23T18:40:34.179067Z","shell.execute_reply":"2024-07-23T18:40:34.177927Z","shell.execute_reply.started":"2024-07-23T18:40:33.111350Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["generator_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","generator_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","\n","def generate_answer(query, context):\n","    input_text = f\"question: {query} context: {context}\"\n","    input_ids = generator_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n","    outputs = generator_model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True)\n","    return generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","def get_node_content(node_id):\n","    for tree in pdf_trees.values():\n","        def find_node(node):\n","            if node['id'] == node_id:\n","                return \" \".join(node['content'])\n","            for child in node['children']:\n","                result = find_node(child)\n","                if result:\n","                    return result\n","        content = find_node(tree)\n","        if content:\n","            return content\n","    return \"\"\n","\n","def rag_pipeline(query):\n","    retrieved_node_ids = hybrid_retrieval(query)\n","    retrieved_contents = [get_node_content(node_id) for node_id in retrieved_node_ids]\n","    reranked_contents = rerank_results(query, retrieved_contents)\n","    context = \" \".join(reranked_contents[:3])  # Use top 3 reranked results as context\n","    answer = generate_answer(query, context)\n","    return answer"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:41:46.057630Z","iopub.status.busy":"2024-07-23T18:41:46.056888Z","iopub.status.idle":"2024-07-23T18:41:50.583177Z","shell.execute_reply":"2024-07-23T18:41:50.582101Z","shell.execute_reply.started":"2024-07-23T18:41:46.057596Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Query: What is BEND THEIR REALITY?\n","Answer: Compromise and concession, even to the truth, feels like defeat\n"]}],"source":["query = \"What is BEND THEIR REALITY?\"\n","answer = rag_pipeline(query)\n","print(f\"Query: {query}\")\n","print(f\"Answer: {answer}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T18:46:36.304529Z","iopub.status.busy":"2024-07-23T18:46:36.303902Z","iopub.status.idle":"2024-07-23T18:46:43.927487Z","shell.execute_reply":"2024-07-23T18:46:43.926402Z","shell.execute_reply.started":"2024-07-23T18:46:36.304499Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5395540,"sourceId":8963910,"sourceType":"datasetVersion"},{"datasetId":5415601,"sourceId":8991352,"sourceType":"datasetVersion"},{"datasetId":5423207,"sourceId":9002470,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
